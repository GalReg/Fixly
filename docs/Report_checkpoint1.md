### 1. Процесс получения данных

**Источник:**
В качестве основного источника данных был использован ZIM-файл `ifixit_ru_all_2025-06.zim`. Это оффлайн-архив русскоязычной версии сайта iFixit, содержащий в себе статьи, руководства по ремонту и описания устройств в формате HTML.

**Процесс извлечения (скрипт `extract_ifixit.py`):**
1.  Скрипт последовательно читает все записи внутри ZIM-архива.
2.  Применяется фильтрация для отбора только релевантного контента: руководств (`Guide/`), статей из вики (`Wiki/`) и описаний устройств (`Device/`).
3.  Для каждой подходящей HTML-страницы запускается парсер (`extract_smart_text`), который целенаправленно извлекает полезную информацию:
    *   Заголовок руководства.
    *   Введение (описание проблемы).
    *   Списки необходимых инструментов и запчастей.
    *   Пошаговые инструкции по ремонту.
4.  Извлеченный и структурированный текст для каждой статьи сохраняется в отдельный `.txt` файл.

### 2. Структура и хранение данных

Данные в проекте хранятся в двух формах:

1.  **Исходные текстовые файлы:**
    *   **Расположение:** папка `/ifixit_data/`.
    *   **Структура:** 54,376 отдельных `.txt` файлов. Каждый файл соответствует одной статье или руководству с iFixit. Текст внутри файлов уже первично структурирован с помощью заголовков-разделителей (`Заголовок:`, `--- ВВЕДЕНИЕ ---`, `--- ШАГИ ---` и т.д.), что облегчает его дальнейшую обработку и улучшает восприятие моделью.

2.  **Векторная база данных:**
    *   **Расположение:** папка `/vectorstore/db_faiss/`.
    *   **Структура:** Индекс FAISS, содержащий в себе текстовые фрагменты (чанки) и их эмбеддинги, полученные с помощью модели `YandexGPTEmbeddings`.
    *   Из-за ограничений на размера файла в 25МБ полную векторную БД невозможно сохранить на GitHub, поэтому можно посмотреть только на один батч (база по 1000 чанков) индекса, папка `/vectorstore/db_faiss/example_batch1/`.

### 3. Сэмпл собранных данных

Ознакомиться с примерами собранных и первично обработанных текстовых файлов можно в репозитории проекта.

*   **Ссылка на папку с несколькими примерами .txt файлов:** `https://github.com/GalReg/Fixly/tree/main/ifixit_data`

### 4. Объём данных

*   **Количество исходных документов:** Из архива было извлечено и сохранено **54,376** релевантных документов (руководств и статей).
*   **Количество чанков для RAG:** После разбиения, исходные документы были преобразованы в **~108 000** чанков, которые легли в основу векторной базы данных.

Статистика по длине документов (в символах)
Всего обработано документов: 54,376
Средняя длина: 2,553
Медианная длина: 1,732
Стандартное отклонение: 2,673
Минимальная длина: 121
Максимальная длина: 35,341

### 5. Подготовка данных для RAG/LLM/агента

Подготовка данных выполнялась в три ключевых шага, реализованных в скриптах `extract_ifixit.py`, `create_vector_db.py` и `merge_vector_db.py`.

1.  **Первичная структуризация (в `extract_ifixit.py`):**
    На этапе парсинга HTML-страниц данные были разделены на логические блоки: заголовок, введение, инструменты, запчасти, шаги.

2.  **Чанкинг (в `create_vector_db.py`):**
    Каждый текстовый документ был разбит на более мелкие фрагменты (чанки) с использованием `RecursiveCharacterTextSplitter`.
    *   **Размер чанка (`chunk_size`):** 2000 символов.
    *   **Перекрытие (`chunk_overlap`):** 400 символов.

3.  **Векторизация и создание базы (в `create_vector_db.py` и `merge_vector_db.py`):**
    *   Каждый текстовый чанк был преобразован в эмбеддинг с помощью модели `YandexGPTEmbeddings`.
    *   Для надежности и отказоустойчивости процесс был разделен на этапы: сначала создавались временные "чекпоинты" базы (по 1000 чанков), а затем они объединялись в единый финальный FAISS-индекс `db_faiss`.

На данный момент все данные собраны, обработаны, векторизованы и сохранены в готовой к использованию векторной базе.
